{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j3aFniNj6dyf"
   },
   "source": [
    "# LX 394 / 594 / 694 Homework 2\n",
    "\n",
    "In this homework we will do some classification and categorization.  It was a long time coming.  We talked through a lot of these things in class, so much of this is kind of just review, and practice doing it yourself.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YO6rRkW7N_24",
    "outputId": "4f6e4372-5982-459b-c43f-b5a4c75f5d67"
   },
   "outputs": [],
   "source": [
    "# Run this first, it makes the autograder go.\n",
    "files = \"https://github.com/bucomplx/lx394s21/raw/main/assets/ipynb/hw2/tests.zip\"\n",
    "!pip install otter-grader && wget $files && unzip -o tests.zip\n",
    "import otter\n",
    "grader = otter.Notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fTsoa4ugUGuT"
   },
   "source": [
    "NLTK comes with a corpus of movie reviews, that has been segmented into positive and negative reviews. Using this, we will try to construct a machine that can guess whether a new review is positive or negative. (\"Sentiment analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-z86EQC1wPnu",
    "outputId": "a75ca344-efa4-43f9-8da1-e229c23a544a"
   },
   "outputs": [],
   "source": [
    "# make NLTK and the movie reviews corpus available\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "nltk.download('movie_reviews')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5DmVpyZpUp83"
   },
   "source": [
    "Different corpora are organized in different ways, but generally they are collections of individual files, often categorized. The movie reviews corpus has many files, one file per review, and categorized into those that are positive and those that are negative. The positive reviews are in the `pos` directory (folder) and the negative reviews are in the `neg` directory. (When writing out a filename, folders are indicated by a `/` character.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "5_pLtworKdTe"
   },
   "outputs": [],
   "source": [
    "# Activity:\n",
    "# categories() is defined for movie_reviews.\n",
    "# this isn't defined for ALL corpora, but it is for this one.\n",
    "# to see that, try typing\n",
    "# movie_reviews.\n",
    "# on the line below and look at what pops up after you type .\n",
    "# We see things like raw, paras, readme, and categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7yc542v6Lwvc",
    "outputId": "37299599-0279-4f03-f4f4-6e9cb66cad64"
   },
   "outputs": [],
   "source": [
    "# Let's get the corpus to tell use what categories it has defined\n",
    "print(movie_reviews.categories())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e-1oXCQRMLWm"
   },
   "source": [
    "**Activity**. When you looked at what `movie_reviews` knew how to do (that is, \"what methods are defined\" in Python parlance), which you got above by typing `movie_reviews.` and looking at the autocompletion popup, you saw `raw` and `categories` and `readme` and some other things.  The `readme` method gives you information about the corpus. Get the corpus to tell you this information, in the same kind of way we got it to tell us what the categories were above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C3yQTYRXLWgX",
    "outputId": "c40b2087-d6ab-4171-d13d-7b73185d396f"
   },
   "outputs": [],
   "source": [
    "# Put a command below that will print out the readme information\n",
    "...\n",
    "\n",
    "# Question: what year was this (v2.0) dataset released?\n",
    "# So the autograder can check it, assign the proper year to v2_year\n",
    "# It should look like v2_year = 1812 (note: that is not the correct year)\n",
    "v2_year = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T4WZ34qtYcUC"
   },
   "source": [
    "We now know what the categories are, but let's print them in a nicer way.  First, we'll count how many there are by asking for the `len()` (length) of the list that `categories()` returned, and then we'll list them out, joined by commas between them. We can do this using `len()` and `format()` and `join()`. This should be kind of familiar, but make sure you understand how these lines work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ky0_CbV3UeEl",
    "outputId": "fef4d625-bbe4-43f8-eaf7-5ea088a96921"
   },
   "outputs": [],
   "source": [
    "print(\"There are {} categories\".format(len(movie_reviews.categories())))\n",
    "print(\"They are: \" + \", \".join(movie_reviews.categories()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "65T3R6gfc9sV"
   },
   "source": [
    "Nice!  That's much easier to read.  Another thing that the corpus can do is give it the names of all the files it contains. We can get that list with `fileids()`. That list is long though.  There are (as we'll see below) 2000 of them. So, printing all 2000 of them out with commas in between would just be an unreadable mess.  We don't need to see them all, we can just look at the first three to get a sense of what they look like. It's not a bad idea to just make sure that the things you're working with (like categories or fileids) look like what you expect them to look like. So this is a kind of sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cIQSXJ3dVhtp",
    "outputId": "4016fdf0-c85f-44fd-a3c7-bf65502cd39a"
   },
   "outputs": [],
   "source": [
    "print(\"There are {} files\".format(len(movie_reviews.fileids())))\n",
    "print(\"The first 3 are: \" + \", \".join(movie_reviews.fileids()[:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WXW50rPxWe0A"
   },
   "source": [
    "Once we did the `...import movie_reviews` step, `movie_reviews` became a known corpus object, that we can interact with.  We can tell the corpus object to give us its categories, as above, with `categories()`, and we can ask it to give us its filenames with `fileids()`. If we want just the filenames for a single category, we can specify it like this: `fileids(categories='pos')` or (if you want the filenames from several categories) `fileids(categories=['neg', 'pos'])`. The `categories=` parameter can take either a string (naming a category) or a list of strings (each of which names a category).\n",
    "\n",
    "The code below will go through the categories, and, for each category, it will call the current category `c` and then execute the indented block.  The first line gets the fileids for all the reviews in category `c` (whatever `c` is on this iteration), and then tells us how many there are and what the first 3 fileids are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MXAheuVTV--m",
    "outputId": "048fcc76-3844-434e-fdcf-0530af15bf73"
   },
   "outputs": [],
   "source": [
    "for c in movie_reviews.categories():\n",
    "  ids = movie_reviews.fileids(categories=c)\n",
    "  print(\"There are {} reviews in category {}.\".format(len(ids), c))\n",
    "  print(\"The first 3 files are: \" + ', '.join(ids[:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Me8uFQRgXdTH"
   },
   "source": [
    "The hypothesis we will pursue first is that we can guess whether a review is positive or negative based on what words it contains. Roughly speaking, a review that contains the word \"boring\" is probably negative, and one that contains the word \"exciting\" is probably positive.\n",
    "\n",
    "So, let's try to work that out.\n",
    "\n",
    "These reviews are just raw text, they have not been processed.  So one thing we want to do right away is convert all the words to lowercase, so we don't consider a word like \"boring\" to be two different words when it is at the beginning of a sentence and when it is in the middle of a sentence.\n",
    "\n",
    "The second thing we want to do is to remove all the super-common/grammatical words and punctuation so that what's left are the more contentful words.  This is all an approximation, but the idea is that `.` and `the` and `and` are not providing useful information about the contents of a review, so we want to filter all those out.  The are, in NLP terminology, \"stopwords.\" And NLTK has a corpus of them (in several languages, we'll use the English ones).\n",
    "\n",
    "So what we are going to do is create a filtered version of these reviews, where the stopwords are removed and the remaining words are lowercased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rO3rxX1bpHQF",
    "outputId": "03f42c9e-f37a-497c-a947-dec4a5332ada"
   },
   "outputs": [],
   "source": [
    "# make the stopwords corpus available.\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x35Xdvkm61pQ"
   },
   "outputs": [],
   "source": [
    "# make the punctuation list available\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jM7-UhXa2KMb"
   },
   "source": [
    "Now that the stopwords and punctuation list have been loaded up, we want to combine them into a list of words to exclude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GIfs7Hcx2Yy1",
    "outputId": "ad8dd871-82d9-493f-9ffe-31cc1631d789"
   },
   "outputs": [],
   "source": [
    "# if we inquire about the fileids in the stopwords corpus,\n",
    "# we see the list of languages that it includes.\n",
    "print(stopwords.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hp7mCgBp2rMG"
   },
   "outputs": [],
   "source": [
    "# the ones we want are the English ones, so we use words() to pull those out.\n",
    "eng_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5B3irGNI20pz",
    "outputId": "f89015af-42ff-4504-bf37-9e77bdec5945"
   },
   "outputs": [],
   "source": [
    "# let's take a look at what we got.  It's a list of words.  They're all lowecase.\n",
    "print(eng_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QLlNzu763AaR"
   },
   "source": [
    "We now want to augment the list of stopwords with \"punctuation words\". Most tokenizers and corpora split off punctuation into their own words, so a sentence in a corpus might be represented like `['I', 'left', '.']`, with the `.` representing its own word.  We want to filter out those punctuation marks as well as the stopwords, so we're planning on making a bigger list of words that contains all the stopwords and the punctuation words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tpgoEUrV3faf",
    "outputId": "6d88ba73-93a3-4b1c-bfb4-abf9562ed24f"
   },
   "outputs": [],
   "source": [
    "# here is a string containing all the relevant punctuation marks.\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3iEyOpxU3mE0",
    "outputId": "7e463553-0a0b-43c4-c43c-05fbcaba3657"
   },
   "outputs": [],
   "source": [
    "# you can go through this string and print each character.\n",
    "# a string can be viewed as a list of characters, so you can\n",
    "# iterate through a string by character just like iterating\n",
    "# through a list.  The code below prints each punctuation character\n",
    "# between square brackets.  The end='' part of the print()\n",
    "# command tells python not to move to the next line but just\n",
    "# keep printing on the same line (if you do not specify the end\n",
    "# parameter, the default is to move to the next line).\n",
    "for c in string.punctuation:\n",
    "  print(' [{}]'.format(c), end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jSgKg72f4lQa"
   },
   "source": [
    "**Question** Create a list of words formed from the punctuation string.  I expect you'll use a list comprehension. Though if you do, take a look at what the list comprehension is doing. There's actually an even easier way to get the same result, but however you get the result is fine.  Your list should look like `['!', '\"', '#', '$', etc.]` once you've formed it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BkQVj9J4476M",
    "outputId": "e8b2266a-ce85-4bbf-8056-907b5bfc635b"
   },
   "outputs": [],
   "source": [
    "punc_words = ...\n",
    "print(punc_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQkq58Yu5OF_"
   },
   "source": [
    "Now that we have `punc_words` and `eng_stopwords` defined, we can create a full list of words to filter out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qs0z6-Zh5SSd",
    "outputId": "e82131ea-0560-4384-fb0d-f32cc8796811"
   },
   "outputs": [],
   "source": [
    "skipwords = eng_stopwords + punc_words\n",
    "print(\"We have {} words to filter out.\".format(len(skipwords)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**\n",
    "\n",
    "These are some submission instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of lx694s21-hw2-beta.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
